{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Speaker-Identification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMMS5tcNgA8w",
        "outputId": "2bdad33f-8bff-4cc1-e44c-69845deae2c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2mAv4sIgsYI",
        "outputId": "1a809e9d-0d91-4a70-bb25-171bbdc1ec26"
      },
      "source": [
        "%cd /content/drive/MyDrive/Voice_Vertification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Voice_Vertification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-nNrGTB-KUo"
      },
      "source": [
        "!pip install comet_ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSZ7odMm-Rw6"
      },
      "source": [
        "from comet_ml import Experiment"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgPtJ2gX-O28"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi41GsVyM-nU"
      },
      "source": [
        "!pip3 install transformers==4.9.2 soundfile datasets==1.11.0 pyctcdecode\n",
        "!pip3 install https://github.com/kpu/kenlm/archive/master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMhXCBLUNBye"
      },
      "source": [
        "from transformers.file_utils import cached_path, hf_bucket_url\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import os, zipfile"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWTi2cYF523A"
      },
      "source": [
        "# SET-UP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJZZp0aqg8mA"
      },
      "source": [
        "TEST_ROOT = 'Zalo_Voice_Verification/Train-Test-Data/dataset/272-M-26/'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5HB-2UTg_mi"
      },
      "source": [
        "import glob \n",
        "import random\n",
        "\n",
        "test_files = glob.glob(os.path.join(TEST_ROOT,'*.wav'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un54-rnlhBWv",
        "outputId": "01e96911-9763-4b08-f560-08136ac3c621"
      },
      "source": [
        "test_file = random.choice(test_files)\n",
        "print(test_file)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zalo_Voice_Verification/Train-Test-Data/dataset/272-M-26/speaker_272-12.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltda9RYRBQgc"
      },
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as tf\n",
        "\n",
        "test , sr = torchaudio.load(test_file)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2g7IEXBSX_"
      },
      "source": [
        "test = tf.Resample(sr,16000)(test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aakP6_khCEpq",
        "outputId": "80310877-2789-4670-9d11-92678cf0c128"
      },
      "source": [
        "tf.MelSpectrogram(sample_rate =  16000, n_fft= 400)(test).shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchaudio/functional/functional.py:433: UserWarning:\n",
            "\n",
            "At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 301])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AlFr7p7kcU4",
        "outputId": "cb10bf00-1d2b-4713-a10b-517f40a084ca"
      },
      "source": [
        "tf.MFCC(sample_rate =16000,n_mfcc=40)(test).shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchaudio/functional/functional.py:433: UserWarning:\n",
            "\n",
            "At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 40, 301])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3HAfoXME67D",
        "outputId": "f7785322-c0e1-4a79-e854-c9d94ab2f4fd"
      },
      "source": [
        "!apt-get install libsox-fmt-all libsox-dev sox > /dev/null\n",
        "! python -m pip install torchaudio > /dev/null\n",
        "! python -m pip install git+https://github.com/facebookresearch/WavAugment.git > /dev/null\n",
        "!pip install ffmpeg-python > /dev/null"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Running command git clone -q https://github.com/facebookresearch/WavAugment.git /tmp/pip-req-build-nzkd1pjd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPV8gMFSFVN9"
      },
      "source": [
        "import augment\n",
        "\n",
        "def augment_wav(signal,sr=16000):\n",
        "    signal = torch.from_numpy(signal)\n",
        "    reverb_signal = augment.EffectChain().reverb(50, 50, 50).channels(1).apply(signal, src_info={'rate': sr})\n",
        "    noise_generator = lambda: torch.zeros_like(reverb_signal).uniform_()\n",
        "    additive_noise_signal = augment.EffectChain().additive_noise(noise_generator, snr=random.uniform(15,25)).apply(reverb_signal, src_info={'rate': sr})\n",
        "    time_dropout_signal =  augment.EffectChain().time_dropout(max_seconds=random.uniform(0.1,0.25)).apply(additive_noise_signal, src_info={'rate': sr})\n",
        "    return time_dropout_signal,sr"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuCJ9WE1OSX-",
        "outputId": "134961d5-18a1-4dd5-d6a3-9c2eac59892f"
      },
      "source": [
        "cache_dir = './cache/'\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"nguyenvulebinh/wav2vec2-base-vietnamese-250h\", cache_dir=cache_dir)\n",
        "lm_file = hf_bucket_url(\"nguyenvulebinh/wav2vec2-base-vietnamese-250h\", filename='vi_lm_4grams.bin.zip')\n",
        "lm_file = cached_path(lm_file,cache_dir=cache_dir)\n",
        "with zipfile.ZipFile(lm_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(cache_dir)\n",
        "lm_file = cache_dir + 'vi_lm_4grams.bin'"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjtCBMceZq9s"
      },
      "source": [
        "pretrained_model = Wav2Vec2ForCTC.from_pretrained(\"nguyenvulebinh/wav2vec2-base-vietnamese-250h\", cache_dir=cache_dir)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrLX3QqgNmy4"
      },
      "source": [
        "def wav2vec(signal, sr):\n",
        "  input_values = processor(\n",
        "        signal[0], \n",
        "        sampling_rate= sr, \n",
        "        return_tensors=\"pt\"\n",
        "  ).input_values\n",
        "  logits = pretrained_model(input_values).logits[0].T\n",
        "  return logits.unsqueeze(dim = 0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8X5G6PLPhux",
        "outputId": "5a7041df-fba0-4d09-c1be-35286d0aa725"
      },
      "source": [
        "def get_embedding(file, type = 'wav2vec', sample_rate =  16000, mode = 'val', n_fft = 400, n_mfcc = 40):\n",
        "  test , sr = torchaudio.load(file)\n",
        "  test = tf.Resample(sr,sample_rate)(test)\n",
        "  if mode == 'train':\n",
        "    test, sr = augment_wav(test.numpy())\n",
        "  if type == 'melspec':\n",
        "    test = tf.MelSpectrogram(sample_rate = sample_rate, n_fft= n_fft)(test)\n",
        "  elif type == 'mfcc':\n",
        "    test = tf.MFCC(sample_rate = sample_rate,n_mfcc = n_mfcc)(test)\n",
        "  elif type == 'wav2vec':\n",
        "    try:\n",
        "      test = wav2vec(test, sample_rate)\n",
        "    except:\n",
        "      test = torch.ones((1,110,110))\n",
        "  if test.shape[0] !=1:\n",
        "    test = test[0].unsqueeze(dim=0)\n",
        "  return test\n",
        "\n",
        "get_embedding(test_file,type = 'wav2vec').shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 110, 187])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEjdN8MEGAcG",
        "outputId": "922449ba-564b-4fb6-9ffb-ca71a49d0af9"
      },
      "source": [
        "DATA_ROOT = 'Zalo_Voice_Verification/Train-Test-Data/dataset'\n",
        "subject_folders = os.listdir(DATA_ROOT)\n",
        "print(len(subject_folders))\n",
        "# test_subject_folder =  os.path.join(DATA_ROOT, random.choice(subject_folders)) "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvVCF15DBsjt",
        "outputId": "14579684-1584-47b9-f2eb-85aedfa7d0e4"
      },
      "source": [
        "paths = glob.glob(DATA_ROOT+'/*/*.wav')\n",
        "file_n = len(paths)\n",
        "print(file_n)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOS8djoYFsKU"
      },
      "source": [
        "def get_prefix(file):\n",
        "  split = file.split('/')\n",
        "  return '/'.join(split[:-1]),split[-2]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlkGSwLiZJDf",
        "outputId": "a0949a04-e275-4364-faee-b044c32c0104"
      },
      "source": [
        "print(get_prefix(test_file))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Zalo_Voice_Verification/Train-Test-Data/dataset/272-M-26', '272-M-26')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECZVwVO_WIvb"
      },
      "source": [
        "import json\n",
        "\n",
        "def read_json(file):\n",
        "  with open(file,'r') as f:\n",
        "    data = json.load(f)\n",
        "  return data\n",
        "\n",
        "def write_json(file,data):\n",
        "  with open(file,'w') as f:\n",
        "    json.dump(data,f,indent = 4)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLmQmchSnBLP"
      },
      "source": [
        "SAVE_TRAIN_PATH = 'train_data.json'\n",
        "SAVE_VAL_PATH = 'test_data.json'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT4ZIsC0WJIo"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def get_data(save_train_file = SAVE_TRAIN_PATH, save_test_file = SAVE_VAL_PATH, test_size = 0.2):\n",
        "    train_total = {}\n",
        "    test_total = {}\n",
        "    test_len = int(file_n * test_size)\n",
        "    train_len = file_n - test_len\n",
        "\n",
        "    for i,subject in tqdm(enumerate(subject_folders)):\n",
        "      paths = glob.glob(os.path.join(DATA_ROOT,subject)+\"/*/*.wav\")\n",
        "      for j,path in enumerate(paths):\n",
        "        if j < int(len(paths) * test_size):\n",
        "          test_total[path] = i\n",
        "        else:\n",
        "          train_total[path] = i\n",
        "      if i == 5:\n",
        "        break\n",
        "        \n",
        "    write_json(save_train_file,train_total)\n",
        "    write_json(save_test_file,test_total)\n",
        "\n",
        "# get_data()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9wa169nNX1h"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwkTjWRjcNXT"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, json_file, resize = None, up_channel= False, type = 'wav2vec', mode = 'train'):\n",
        "        super(SpeechDataset, self).__init__()\n",
        "        self.data = list(read_json(json_file).items())\n",
        "        if type == 'mfcc':\n",
        "          max_sequence_len = 40\n",
        "        elif type == 'melspec':\n",
        "          max_sequence_len = 128\n",
        "        elif type == 'wav2vec':\n",
        "          max_sequence_len = 110\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.resize = resize\n",
        "        self.mode = mode\n",
        "        self.up_channel = up_channel\n",
        "        self.type = type\n",
        "        \n",
        "\n",
        "    def preprocess(self, em):\n",
        "        if self.up_channel:\n",
        "          em = np.stack((em,)*3, axis=-1)\n",
        "        if self.resize is not None:\n",
        "          em = cv2.resize(em.T,self.resize).T\n",
        "          em = np.expand_dims(em, axis =0)\n",
        "        return torch.FloatTensor(em)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.data[idx]\n",
        "        em = get_embedding(path, type = self.type, mode = self.mode).detach().numpy()\n",
        "        em = self.pad(em)\n",
        "        em = self.preprocess(em)\n",
        "        return self.normalize(em), torch.LongTensor([int(label)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def normalize(self,em,mean=0.5,std=0.5):\n",
        "        # return (em - em.min())/(em.max()-em.min())\n",
        "        return em\n",
        "\n",
        "    def pad(self,em): \n",
        "        if em.shape[2] < self.max_sequence_len:\n",
        "            pad = np.zeros((em.shape[0],em.shape[1],self.max_sequence_len - em.shape[2]))\n",
        "            em = np.concatenate((em, pad), axis=2)\n",
        "        else:\n",
        "            em = em[:,:,:self.max_sequence_len] \n",
        "        assert em.shape == (1,self.max_sequence_len,self.max_sequence_len)\n",
        "        return em"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff1V5xPtmZrx"
      },
      "source": [
        "def get_loader(bs, nw, siz):  \n",
        "\n",
        "  train_dataset = SpeechDataset(json_file=SAVE_TRAIN_PATH, resize = siz, up_channel=False, mode='val')\n",
        "  test_dataset = SpeechDataset(json_file=SAVE_VAL_PATH, resize = siz, up_channel=False, mode='val')\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS\n",
        "  )\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "          dataset=test_dataset,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          num_workers=NUM_WORKERS\n",
        "  )\n",
        "  print(\"DONE!\")\n",
        "  return train_loader,test_loader"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow47YZEaNb6C"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71l936VkneIS"
      },
      "source": [
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F\n",
        "import torch \n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_depth, depth, first=False):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.first = first\n",
        "        self.conv1 = nn.Conv2d(in_depth, depth, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(depth)\n",
        "        self.lrelu = nn.LeakyReLU(0.01)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.conv2 = nn.Conv2d(depth, depth, kernel_size=3, stride=3, padding=1)\n",
        "        self.conv11 = nn.Conv2d(in_depth, depth, kernel_size=3, stride=3, padding=1)\n",
        "        if not self.first :\n",
        "            self.pre_bn = nn.BatchNorm2d(in_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        prev = x\n",
        "        prev_mp =  self.conv11(x)\n",
        "        if not self.first:\n",
        "            out = self.pre_bn(x)\n",
        "            out = self.lrelu(out)\n",
        "        else:\n",
        "            out = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.lrelu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv2(out)\n",
        "        out = out + prev_mp\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.block1 = ResNetBlock(32, 32,  True)\n",
        "        self.mp = nn.MaxPool2d(3, stride=3, padding=1)\n",
        "        self.block2 = ResNetBlock(32, 32,  False)\n",
        "        self.block3 = ResNetBlock(32, 32,  False)\n",
        "        self.block4= ResNetBlock(32, 32, False)\n",
        "        self.block5= ResNetBlock(32, 32, False)\n",
        "        self.block6 = ResNetBlock(32, 32, False)\n",
        "        self.block7 = ResNetBlock(32, 32, False)\n",
        "        self.block8 = ResNetBlock(32, 32, False)\n",
        "        self.block9 = ResNetBlock(32, 32, False)\n",
        "        self.block10 = ResNetBlock(32, 32, False)\n",
        "        self.block11 = ResNetBlock(32, 32, False)\n",
        "        self.lrelu = nn.LeakyReLU(0.01)\n",
        "        self.bn = nn.BatchNorm2d(32)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(32, 128)\n",
        "        self.fc2 = nn.Linear(128, 6)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.mp(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.block4(out)\n",
        "        out = self.mp(out)\n",
        "        out = self.block5(out)\n",
        "        out = self.block6(out)\n",
        "        out = self.mp(out)\n",
        "        out = self.block7(out)\n",
        "        out = self.block8(out)\n",
        "        out = self.mp(out)\n",
        "        out = self.block9(out)\n",
        "        out = self.block10(out)\n",
        "        out = self.mp(out)\n",
        "        out = self.block11(out)\n",
        "        out = self.bn(out)\n",
        "        out = self.lrelu(out)\n",
        "        out = self.mp(out)\n",
        "        out = out.view(batch_size, -1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.lrelu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.logsoftmax(out)\n",
        "        return out"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8StjKxLoEVN",
        "outputId": "dd995da9-03fe-4e1d-d59e-2207a3994385"
      },
      "source": [
        "in1 = torch.randn((2,1,110,110))\n",
        "model = ResModel()\n",
        "out = model(in1)\n",
        "# summary(model, (1, 110, 110),device = 'cpu')\n",
        "print(out.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc69aiE0751f"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ-oTTJm992U"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjsK7XihxaIx"
      },
      "source": [
        "# Create an experiment with your api key\n",
        "experiment = Experiment(\n",
        "    api_key=\"Your api key\",\n",
        "    project_name=\"Speech Indentification V3\",\n",
        "    workspace=\"maxph2211\",\n",
        ")\n",
        "\n",
        "hyper_params = {\n",
        "    \"re_siz\": 112,\n",
        "    \"n_fft\" : 400,\n",
        "    \"n_mfcc\" : 40,  \n",
        "    \"batch_size\": 64,\n",
        "    \"num_epochs\": 300,\n",
        "    \"learning_rate\": 0.0001\n",
        "}\n",
        "\n",
        "experiment.log_parameters(hyper_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvP-uj0wsEK7"
      },
      "source": [
        "device = 'cuda'\n",
        "EPOCH_N = hyper_params['num_epochs']\n",
        "LEARNING_RATE = hyper_params['learning_rate']\n",
        "ckpt = 'resnet.pth'\n",
        "BEST_LOSS = np.inf\n",
        "BATCH_SIZE = hyper_params['batch_size']\n",
        "NUM_WORKERS = 2\n",
        "re_siz =  None #(hyper_params['siz'],hyper_params['siz']) "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLQE70wf6rZR",
        "outputId": "e41aa259-7978-4cb2-9c8d-9f93bc6237d9"
      },
      "source": [
        "train_loader,test_loader = get_loader(BATCH_SIZE, NUM_WORKERS, re_siz)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baQl7WlCuyyb"
      },
      "source": [
        "# model.load_state_dict(torch.load(ckpt))\n",
        "model = model.to(device)\n",
        "# criterion = nn.CrossEntropyLoss().to(device)\n",
        "criterion = nn.NLLLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=5, verbose=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI9cOb2FnV1A"
      },
      "source": [
        "def train_epoch(train_loader, model, criterion, optimizer):\n",
        "  model.train()\n",
        "  train_loss_epoch = 0\n",
        "  train_acc_epoch = []\n",
        "  for em,label in tqdm(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    em = em.to(device)\n",
        "    label = label.to(device)\n",
        "    out = model(em)\n",
        "    loss = criterion(out,label.squeeze(dim=1))\n",
        "    train_loss_epoch+=loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, predict = out.max(dim=1)\n",
        "    train_acc_epoch.append(accuracy_score(predict.cpu().numpy(), label.cpu().numpy()))\n",
        "  return sum(train_acc_epoch)/len(train_acc_epoch), train_loss_epoch, model , optimizer\n",
        "\n",
        "\n",
        "def val_epoch(test_loader, model, criterion, optimizer):\n",
        "  model.eval()\n",
        "  val_loss_epoch = 0\n",
        "  val_acc_epoch = []\n",
        "  with torch.no_grad():\n",
        "    for em,label in tqdm(test_loader):\n",
        "      em = em.to(device)\n",
        "      label = label.to(device)\n",
        "      out = model(em)\n",
        "      loss = criterion(out,label.squeeze(dim=1))\n",
        "      val_loss_epoch+=loss.item()\n",
        "      _, predict = out.max(dim=1)\n",
        "      val_acc_epoch.append(accuracy_score(predict.cpu().numpy(), label.cpu().numpy()))\n",
        "    return sum(val_acc_epoch)/len(val_acc_epoch), val_loss_epoch\n",
        "\n",
        "\n",
        "def save_checkpoint(val_loss_epoch,ckpt,model):\n",
        "  global BEST_LOSS\n",
        "  if val_loss_epoch < BEST_LOSS:\n",
        "    BEST_LOSS = val_loss_epoch\n",
        "    torch.save(model.state_dict(),ckpt)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw3iUc2UorF_"
      },
      "source": [
        "for epoch in range(EPOCH_N):\n",
        "    with experiment.train():\n",
        "      mean_train_acc, train_loss_epoch, model , optimizer = train_epoch(train_loader, model, criterion, optimizer)\n",
        "\n",
        "      experiment.log_metrics({\n",
        "            \"loss\": train_loss_epoch,\n",
        "            \"acc\": mean_train_acc\n",
        "      }, epoch=epoch)\n",
        "\n",
        "    with experiment.test():\n",
        "      mean_val_acc, val_loss_epoch = val_epoch(test_loader, model, criterion, optimizer)\n",
        "      save_checkpoint(val_loss_epoch,ckpt,model)\n",
        "      scheduler.step(val_loss_epoch)\n",
        "      experiment.log_metrics({\n",
        "            \"loss\": val_loss_epoch,\n",
        "            \"acc\": mean_val_acc\n",
        "      }, epoch=epoch)\n",
        "    \n",
        "    print(\"EPOCH: \", epoch+1,\" - TRAIN_LOSS: \", train_loss_epoch,\" - TRAIN_ACC: \",mean_train_acc, \" || VAL_LOSS: \", val_loss_epoch, \" - VAL_ACC: \", mean_val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBaVWHdouLpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d1fe1b-69e0-421f-c155-cfc08ae9673d"
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.ml/maxph2211/speech-indentification-v3/5fc553f1812c4aaebb69e0f5dd990290\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     test_acc [29]   : (0.10677083333333333, 0.2109375)\n",
            "COMET INFO:     test_loss [29]  : (3.531217575073242, 3.565745234489441)\n",
            "COMET INFO:     train_acc [29]  : (0.14192708333333334, 0.23828125)\n",
            "COMET INFO:     train_loss [53] : (1.6536426544189453, 14.366368532180786)\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     batch_size    : 64\n",
            "COMET INFO:     learning_rate : 0.0001\n",
            "COMET INFO:     n_fft         : 400\n",
            "COMET INFO:     n_mfcc        : 40\n",
            "COMET INFO:     num_epochs    : 300\n",
            "COMET INFO:     siz           : 112\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     model graph         : 1\n",
            "COMET INFO:     notebook            : 1\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Uploading 1 metrics, params and output messages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DoSRgYU9OHD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}